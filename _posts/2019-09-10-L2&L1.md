---
layout: post
title: 为什么L1正则项产生稀疏的权重，L2正则项产生相对平滑的权重
thumbnail: "assets/img/thumbnails/desk-L1L2Norm.png"
image: "assets/img/thumbnails/desk-L1L2Norm.png" #seo tag
tags: [Machine Learning, Deep Learning, Markdown]
color: brown
---

# L1和L2正则化的区别

- L1 和L2正则项的定义如下：
$$
L1 = \sum_{i} |w_i|\\
L2 = \sum_{i} (w_i)^2
$$
- 首先我们先计算一下他们对应的导数，导入如下所示：
$$
\frac{\partial L1}{\partial w_i} = 1 or -1 \rightarrow w^{t+1}_i = w^t_i + \eta {(-1 or 1)}\\
\frac{\partial L2}{\partial w_i} = w_i \rightarrow w^{t+1}_i = w^t_i + \eta w_i
$$
- 所以我们看到L1每次更新的时候会更新一个定值，那么若干次迭代之后，权重就有可能减少为0。但是L2每个更新的时候更新的值的大小和$w_i$ 的值是有关系的。当$w_i$ 趋近与0时，那么对应的导数值也会更新，所以他会不停的接近0，但并不会是0。此外，我们还可以得到，L2相对L1更稳定一些。
- L1 产生0的权重也可以起到特征选择的作用，假设我们有$X_0...X_i...X_n$ n个特征，通过分配不同的权重$w_0...w_i...w_n$，然后使用L1 来做特征选择。因为我们使用L1正则，会比较偏向于将某些w设置为0，从而达到了特征选择的作用。
- L2 可以迅速产生接近0的权值，但并不是0，所以会比较平滑。
- 此外，我们还可以从几何的角度来理解。
	- 假设我们的Loss函数是$(y - wx)^2$, 那么我们的几何解释如下图所示：
	![img](https://vimsky.com/wp-content/uploads/2015/08/Screen-Shot-2015-08-26-at-21.56.02.png)
	- 其中左图表示L1，右图表示L2。绿色代表的是loss的等高线，$w_1, w_2$ 在L1中的取值空间如左图的菱形所示。在L2中的取值空间如右图的圆形所示。从等高线和取值空间的交点可以看到L1更容易倾向一个权重偏大一个权重为0。L2更容易倾向权重都较小。
	- 为什么等高线是类似圆形？以linear regression举例，我们的目标函数$E(w) = (y - w^Tx)^2$，可以看到E是关于w的平方函数，所以是类似圆形(椭圆形)。
- [主要参考](https://vimsky.com/article/969.html)